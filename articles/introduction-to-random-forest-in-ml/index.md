A random forest is a supervised machine learning algorithm that is constructed from decision tree algorithms. This algorithm is applied in various industries such as banking and e-commerce to predict behavior and outcomes. 

This article provides an overview of the random forest algorithm and how it works. The article presents meaningful information, such as the algorithm's features and how it is employed in real-life applications. It also highlights the advantages and disadvantages of this algorithm. 

### What is a Random Forest?
A random forest is a machine learning technique that is used to solve regression and classification problems. It utilizes ensemble learning, which is a technique that combines many classifiers to provide solutions to complex problems. 

A random forest algorithm consists of many decision trees. The 'forest' generated by the random forest algorithm is trained through bagging or bootstrap aggregating. Bagging is an ensemble meta-algorithm that improves the accuracy and stability of machine learning algorithms.

The algorithm (random forest) establishes the outcome based on the predictions of the trees. It predicts by taking the average of the output of various decision trees. Increasing the number of trees increases the precision of the outcome. 

A random forest eradicates the limitations of a decision tree algorithm. It reduces the overfitting of datasets and increases precision. It generates predictions without requiring many configurations in packages (like [scikit-learn](https://en.wikipedia.org/wiki/Scikit-learn)).

### Features of a Random Forest Algorithm
* It is more accurate than the decision tree algorithm.
* It provides an effective way of handling missing data.
* It can produce a reasonable prediction without hyper-parameter tuning.
* It solves the problem of overfitting in decision trees.
* In every random forest tree, a subset of features is selected randomly at the node's splitting point.

### How Random Forest Algorithm Works
Decision trees are the building blocks of a random forest algorithm. A decision tree is a decision support technique that forms a tree-like structure. An overview of decision trees will help us understand how random forest algorithms work. 

A decision tree consists of three components: a root node, decision nodes, and leaf nodes. A decision tree algorithm divides a training dataset into branches, which further segregate into other branches. This sequence continues until a leaf node is attained. The leaf node cannot be segregated further.  

The nodes in the decision tree represent attributes that are used for predicting the outcome. Decision nodes provide a link to the leaves. The following diagram shows the three types of nodes in a decision tree. 

![Decision Tree Nodes](/engineering-education/introduction-to-random-forest-in-machine-learning/decision-tree-nodes.png)

[Image Source: Tutorials and Example](https://www.tutorialandexample.com/wp-content/uploads/2019/10/Decision-Trees-Root-Node.png)

Let's take a simple example of how a decision tree works. Suppose we want to predict if a customer will buy a mobile phone or not. The features of the phone form the basis of his decision. This analysis can be presented in a decision tree diagram. The root node and decision nodes of the decision represent the features of the phone mentioned above. The leaf node represents the final output, either *buying* or *not buying*. The main features that determine the choice include the price, internal storage, and Random Access Memory (RAM). The decision tree will appear as follows.

![Example of Decision Tree](/engineering-education/introduction-to-random-forest-in-machine-learning/example-of-decision-tree.png)

[Image Source: Simplilearn](https://www.simplilearn.com/ice9/free_resources_article_thumb/phone-price.JPG)

The main difference between the random forest algorithm and the decision tree algorithm is that in the former, establishing root nodes and segregating nodes is done randomly. The random forest algorithm uses the bagging method to generate the required prediction. 

Bagging involves using different samples of data (training data) rather than just one sample. A training dataset comprises observations and features that are used for making predictions. The decision trees produce different outputs, depending on the training data fed to the rain forest algorithm. These outputs will be ranked, and the highest will be chosen as the final output. 

Our first example can still be used to explain how random forests work. Instead of having a single decision tree, the rain forest will have many decision trees. Let's assume we have only four decision trees. In this case, the training data comprising the phone's observations and features will be divided into four root nodes. 

The root nodes could represent four features that could influence the customer's choice (price, internal storage, camera, and RAM). The random forest will split the nodes by selecting features randomly. The final prediction will be selected based on the outcome of the four trees. The outcome chosen by most decision trees will be the final choice. If three trees predict *buying*, and one tree predicts *not buying*, then the final prediction will be *buying*. In this case, it is predicted that the customer will buy the phone.

### Classification in Random Forest
Classification in random forests employs an ensemble methodology to attain the outcome. The training data is fed to train various decision trees. This dataset consists of observations and features that will be selected randomly during the splitting of nodes.

A rain forest system relies on various decision trees. Every decision tree consists of a root node, decision nodes, and leaf nodes. The leaf node of each tree is the final output produced by that specific decision tree. The selection of the final output follows the majority-voting system. In this case, the output chosen by the majority of the decision trees becomes the final output of the rain forest system. The diagram below shows a simple random forest classifier. 

![Random Forest Classifier](/engineering-education/introduction-to-random-forest-in-machine-learning/random-forest-classifier.png)

[Image Source: Medium](https://miro.medium.com/max/5752/1*5dq_1hnqkboZTcKFfwbO9A.png)

Let's take an example of a training dataset consisting of various fruits such as bananas, apples, pineapples, and mangoes. The random forest classifier divides this dataset into subsets. These subsets are given to every decision tree in the random forest system. each decision tree produces its specific output. For example, the prediction for trees 1 and 2 is *apple*. Another decision tree (n) has predicted *banana* as the outcome. The random forest classifier collects the majority voting to provide the final prediction. The majority of the decision trees have chosen *apple* as their prediction. This makes the classifier to choose *apple* as the final prediction. 

![Example of Random Forest Classifier](/engineering-education/introduction-to-random-forest-in-machine-learning/example-of-random-forest-classifier.png)

[Image Source: Javatpoint](https://static.javatpoint.com/tutorial/machine-learning/images/random-forest-algorithm2.png)

### Applications of Random Forest
Some of the applications of the random forest include:

#### Banking
Random forest is used in banking to predict the creditworthiness of a loan applicant. This helps the lending institution to make a good decision on whether to give the customer the loan or not. Banks also use the random forest algorithm to detect fraudsters. 

#### Health care
Health professionals use random forest systems to diagnose patients. Patients are diagnosed by assessing their previous medical history. Past medical records are reviewed to establish the right dosage for the patients. 

#### Stock Market
Financial analysts use it to identify potential markets for stocks. It also enables them to identify the behavior of stocks. 

#### E-commerce
Through rain forest algorithms, e-commerce vendors can predict the preference of customers based on past consumption behavior. 

### Advantages of Random Forest
* It can perform both regression and classification tasks.
* A random forest produces good predictions that can be understood easily. 
* It can handle large datasets efficiently. 
* The random forest algorithm provides a higher level of accuracy in predicting outcomes than the decision tree algorithm.

### Disadvantages of Random Forest
* In random forest, more resources are required for computation.
* It consumes more time compared to a decision tree algorithm.

### Conclusion
The rain forest algorithm is a machine learning algorithm that is easy to use and flexible. It uses ensemble learning, which enables organizations to solve classification and regression problems. 

This is an ideal algorithm for developers because it solves the problem of overfitting of datasets. It is a very resourceful tool for making accurate predictions needed in strategic decision making in organizations. 

### Resources

[Dev Skrol](https://devskrol.com/index.php/2020/07/26/random-forest-how-random-forest-works/)

[Free Code Camp](https://www.freecodecamp.org/news/how-to-use-the-tree-based-algorithm-for-machine-learning/)


---
Peer Review Contributions by: [Lalithnarayan C](/engineering-education/authors/lalithnarayan-c/)
